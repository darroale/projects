#PHASE 1: Stathead_Individual_Game_Crawl TEAM

import os
import time
import shutil
from selenium import webdriver
from datetime import datetime
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager

# Start the timer since this takes a bit longer to gather data
start_time = time.time()

# Specify the download and destination folders
download_folder = "/Users/alexdarrow/Downloads"
destination_folder = "/Users/alexdarrow/Desktop/programming/jupyter_notebooks/nfl_scraping_project/data_2.0/stathead_individual_game_crawl/individual_team_game_log_data"
destination_folder_abrr = "individual_team_game_log_data"

# Set up Chrome options to specify download directory
chrome_options = webdriver.ChromeOptions()
prefs = {"download.default_directory": download_folder}
chrome_options.add_experimental_option("prefs", prefs)

# Set up the WebDriver using ChromeDriverManager
service = Service(ChromeDriverManager().install())
driver = webdriver.Chrome(service=service, options=chrome_options)

# Login URL
login_url = "https://stathead.com/users/login.cgi"
driver.get(login_url)

# Wait for the login page to load
wait = WebDriverWait(driver, 10)

# Find the login form elements
username_field = wait.until(EC.element_to_be_clickable((By.NAME, "username")))
password_field = driver.find_element(By.NAME, "password")
login_button = driver.find_element(By.ID, "sh-login-button")

# Enter login credentials
username_field.send_keys("adarro5302@gmail.com")  # Replace with your username
password_field.send_keys("Con97set!")  # Replace with your password

# Submit the login form
login_button.click()

# Wait for successful login
time.sleep(5)

# Define the base URL for pagination
base_url = "https://stathead.com/football/team-game-finder.cgi?request=1&order_by=date&team_game_max=17&year_max=2024&comp_type=reg&week_num_season_max=22&timeframe=seasons&week_num_season_min=1&team_game_min=1&year_min=2021&match=team_game&cstat[1]=points&ccomp[1]=gt&cval[1]=0&cstat[2]=pass_cmp&ccomp[2]=gt&cval[2]=0&cstat[3]=rush_att&ccomp[3]=gt&cval[3]=0&cstat[4]=tot_yds&ccomp[4]=gt&cval[4]=0&cstat[5]=penalties&ccomp[5]=gt&cval[5]=0&cstat[6]=first_down&ccomp[6]=gt&cval[6]=0&cstat[7]=all_td_team&ccomp[7]=gt&cval[7]=0&cstat[8]=punt&ccomp[8]=gt&cval[8]=0&cstat[9]=kick_ret_td&ccomp[9]=gt&cval[9]=0&cstat[10]=vegas_line&ccomp[10]=gt&cval[10]=-500&offset="

# Start offset loop
# Update this loop offset if the code times out and you need to start up again in the middle of running
offset = 0
while True:
    # Generate URL with the current offset
    url = f"{base_url}{offset}"
    driver.get(url)
    time.sleep(5)  # Allow time for the page to load
    
    # Check if table data exists on the page
    try:
        export_data_dropdown = wait.until(EC.element_to_be_clickable((By.XPATH, "//span[text()='Export Data']/ancestor::li[@class='hasmore']")))
    except Exception:
        print(f"No more data at offset {offset}. Exiting loop.")
        break

    # Click the dropdown to reveal export options
    export_data_dropdown.click()

    # Select the "Get table as Excel Workbook" button
    excel_button = wait.until(EC.element_to_be_clickable((By.XPATH, "//button[text()='Get as Excel Workbook']")))
    excel_button.click()

    # Wait for the download to complete
    time.sleep(10)

    # Find the most recent file in the download folder
    downloaded_file = max([f for f in os.listdir(download_folder)], key=lambda f: os.path.getctime(os.path.join(download_folder, f)))

    # Move and rename the downloaded file
    file_name_without_extension, file_extension = os.path.splitext(downloaded_file)
    new_file_name_with_html = f"data_offset_{offset}.html"
    shutil.move(os.path.join(download_folder, downloaded_file), os.path.join(destination_folder, new_file_name_with_html))

    print(f"Downloaded and moved data for offset {offset} to {os.path.join(destination_folder_abrr, new_file_name_with_html)}")

    # Increment offset by 200 for the next iteration
    offset += 200

# Close the browser
driver.quit()

# End the timer
end_time = time.time()

print("Data extraction completed.")

# Calculate and print runtime
runtime = end_time - start_time
print(f"Runtime: {runtime / 60:.2f} minutes")

# Print completing date
completion_time = datetime.now()
print(f"Code execution completed at: {completion_time.strftime('%Y-%m-%d %H:%M:%S')}")

