{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3265e8a5-3b95-42ee-9208-9d0c654636d9",
   "metadata": {},
   "source": [
    "### The Game Plan\n",
    "---\n",
    "\n",
    "##### **Objective**:\n",
    "Automate process of pulling current and historical NFL data at game level from stathead for mySQL analysis, Tableau visualizations, and python predictions.\n",
    "- *Note for this to work you'll need:*\n",
    "    1. Your own stathead account at ~5-10/mo for at least NFL stats otherwise you can't pull data in mass.\n",
    "    2. When connecting to mySQL in the last step, the tables need to already be premade in your mySQL database. Typically after the second phase, I'll pull in the file from the csv exported into mySQL and create the table with the wizard. You can then drop the data or keep it. This now gives you a framework to connect to for step 3.\n",
    "\n",
    "##### **Data Description**: \n",
    "- Subject to change but currently I am pulling 3 categories of data. They are team, opponent and advanced level statistics aggregated at game level.\n",
    "    1. *Team:* Perspective of who you are rooting for to win. The data contains offense, special teams, vegas odds, some defense and opponent stats\n",
    "    2. *Opponent:* Perspective of who you are rooting against. The data contains very similar stats as the team tables, however, for the opponents. These stats are important for gaging defensive performance of the team\n",
    "    3. *Advanced:* Statheads own in-house special statistics on the offensive and defensive side of teams. For the purposes of predictions\n",
    "##### **Process Overview**: \n",
    "- Below is a brief description of what is happening behind the scenes. Instead of Eveything occuring simultaneously, I've split it into 3 phases.\n",
    "    1. The first phase calls a script I created titled `stathead_individual_game_crawl`. Here it will loop through all possible pages of data for the below 3 url types and pull till there is no more pages to pull data from. The downloads are sent to a specified folder as HTML files.\n",
    "    2. The second phase is titled `stathead_HTML_to_CVS_export_and_cleanse.` Here the HTML files downloaded from stathead are imported back into python, combined into one table for each set of data (team, opponent, advanced), nulls are updated to 0's, finally each set of data is exported into separate CSVs.\n",
    "    3. The last phase is titled `stathead_csv_to_mySQL_export` once the CSVs are cleaned in your specified folder they are then reimported one last time into a dataframe. A connection is then made to mySQL where the individual dataframes are then added into their own pre-made tables. \n",
    "\n",
    "---\n",
    "\n",
    "#### Website Examples (Copy to see structure)\n",
    "\n",
    "- **Team URL**: [Stathead - Individual Team Game Finder](https://stathead.com/football/team-game-finder.cgi?request=1&order_by=date&team_game_max=17&year_max=2024&comp_type=reg&week_num_season_max=22&timeframe=seasons&week_num_season_min=1&team_game_min=1&year_min=2021&match=team_game&cstat[1]=points&ccomp[1]=gt&cval[1]=0&cstat[2]=pass_cmp&ccomp[2]=gt&cval[2]=0&cstat[3]=rush_att&ccomp[3]=gt&cval[3]=0&cstat[4]=tot_yds&ccomp[4]=gt&cval[4]=0&cstat[5]=penalties&ccomp[5]=gt&cval[5]=0&cstat[6]=first_down&ccomp[6]=gt&cval[6]=0&cstat[7]=all_td_team&ccomp[7]=gt&cval[7]=0&cstat[8]=punt&ccomp[8]=gt&cval[8]=0&cstat[9]=kick_ret_td&ccomp[9]=gt&cval[9]=0&cstat[10]=vegas_line&ccomp[10]=gt&cval[10]=-500&offset=0)\n",
    "\n",
    "- **Opponent URL (IP)**: [Stathead - Individual Opponent Game Finder](https://stathead.com/football/team-game-finder.cgi?request=1&team_game_max=17&week_num_season_min=1&timeframe=seasons&match=team_game&year_max=2024&order_by=date&year_min=2021&team_game_min=1&week_num_season_max=22&comp_type=reg&cstat[1]=pass_cmp_opp&ccomp[1]=gt&cval[1]=0&cstat[2]=rush_att_opp&ccomp[2]=gt&cval[2]=0&cstat[3]=tot_yds_opp&ccomp[3]=gt&cval[3]=0&cstat[4]=first_down_opp&ccomp[4]=gt&cval[4]=0&cstat[5]=all_td_opp&ccomp[5]=gt&cval[5]=0&cstat[6]=def_tgt_yds_per_att&ccomp[6]=gt&cval[6]=0&offset=0)\n",
    "\n",
    "- **Advanced URL**: [Stathead - Individual Advanced Game Finder](https://stathead.com/football/team-game-finder.cgi?request=1&year_min=2021&match=team_game&comp_type=reg&team_game_max=17&team_game_min=1&week_num_season_min=1&year_max=2024&timeframe=seasons&order_by=date&week_num_season_max=22&cstat[1]=pass_target_yds&ccomp[1]=gt&cval[1]=-600&cstat[2]=pass_batted_passes&ccomp[2]=gt&cval[2]=-600&cstat[3]=pocket_time&ccomp[3]=gt&cval[3]=-600&cstat[4]=pass_rpo&ccomp[4]=gt&cval[4]=-600&cstat[5]=rec_yac&ccomp[5]=gt&cval[5]=-600&cstat[6]=rush_yds_before_contact&ccomp[6]=gt&cval[6]=-600&cstat[7]=rush_yds_diff&ccomp[7]=gt&cval[7]=-600&offset=0)\n",
    "\n",
    "    - **Pattern**: The `offset` value increments by **200**. The code will increase the `offset` of the url during the looping process to capture all pages of data and exit the loop once a page has no more data to pull.\n",
    "\n",
    "---\n",
    "\n",
    "#### Criteria for Data Pull\n",
    "\n",
    "- **Seasons**: 2021-2024, Regular Season.\n",
    "- **Team Stats Conditions**:\n",
    "  - Penalties ≥ 0\n",
    "  - Rushing Att ≥ 0\n",
    "  - Kickoff Return TD ≥ 0\n",
    "  - Total Yardage ≥ 0\n",
    "  - Points For ≥ 0\n",
    "  - Punts ≥ 0\n",
    "  - First Downs ≥ 0\n",
    "  - Passes Completed ≥ 0\n",
    "  - Point Spread ≥ -500\n",
    "  - Touchdowns ≥ 0\n",
    "  - Sorted By DESC date\n",
    "\n",
    "- **Opponent Stats Conditions (IP)**:\n",
    "    - DADOT >= 0\n",
    "    - Opponent 1st Downs >= 0\n",
    "    - Opponent Touchdowns >= 0\n",
    "    - Opponent Total Yards >= 0\n",
    "    - Opponent Rushing Attempts >= 0\n",
    "    - Opponent Passes Completed >= 0\n",
    "  \n",
    "- **Advanced Stats Conditions**:\n",
    "    - Rushing Yards Margin >= -600\n",
    "    - RPO Plays >= -600\n",
    "    - Yards After Catch >= -600\n",
    "    - Passes Batted >= -600\n",
    "    - Average Pocket Time >= -600\n",
    "    - Rushing Yards Before Contact** >= -600\n",
    "    - Intended Air Yards >= -600\n",
    "\n",
    "#### Reminders\n",
    "*Make sure when pulling data from each category the row count comes out the same upon import into mySQL. Since null values are updated to 0 and these stats on stathead are aggregated at game level, there should not be discrpancies since every team plays the same amount of games every year and my criteria shouldn't exclude any particular game.*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabd63ef-6755-43a8-b211-e8a3b6de2cac",
   "metadata": {},
   "source": [
    "#### PHASE 1: Stathead_Individual_Game_Crawl TEAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "148210b0-42a6-493d-9c52-961fb6edb9f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded and moved data for offset 0 to individual_team_game_log_data/data_offset_0.html\n",
      "Downloaded and moved data for offset 200 to individual_team_game_log_data/data_offset_200.html\n",
      "Downloaded and moved data for offset 400 to individual_team_game_log_data/data_offset_400.html\n",
      "Downloaded and moved data for offset 600 to individual_team_game_log_data/data_offset_600.html\n",
      "Downloaded and moved data for offset 800 to individual_team_game_log_data/data_offset_800.html\n",
      "Downloaded and moved data for offset 1000 to individual_team_game_log_data/data_offset_1000.html\n",
      "Downloaded and moved data for offset 1200 to individual_team_game_log_data/data_offset_1200.html\n",
      "Downloaded and moved data for offset 1400 to individual_team_game_log_data/data_offset_1400.html\n",
      "Downloaded and moved data for offset 1600 to individual_team_game_log_data/data_offset_1600.html\n",
      "Downloaded and moved data for offset 1800 to individual_team_game_log_data/data_offset_1800.html\n",
      "Downloaded and moved data for offset 2000 to individual_team_game_log_data/data_offset_2000.html\n",
      "No more data at offset 2200. Exiting loop.\n",
      "Data extraction completed.\n",
      "Runtime: 4.67 minutes\n",
      "Code execution completed at: 2024-12-05 16:09:51\n",
      "Downloaded and moved data for offset 0 to individual_opponent_game_log_data/data_offset_0.html\n",
      "Downloaded and moved data for offset 200 to individual_opponent_game_log_data/data_offset_200.html\n",
      "Downloaded and moved data for offset 400 to individual_opponent_game_log_data/data_offset_400.html\n",
      "Downloaded and moved data for offset 600 to individual_opponent_game_log_data/data_offset_600.html\n",
      "Downloaded and moved data for offset 800 to individual_opponent_game_log_data/data_offset_800.html\n",
      "Downloaded and moved data for offset 1000 to individual_opponent_game_log_data/data_offset_1000.html\n",
      "Downloaded and moved data for offset 1200 to individual_opponent_game_log_data/data_offset_1200.html\n",
      "Downloaded and moved data for offset 1400 to individual_opponent_game_log_data/data_offset_1400.html\n",
      "Downloaded and moved data for offset 1600 to individual_opponent_game_log_data/data_offset_1600.html\n",
      "Downloaded and moved data for offset 1800 to individual_opponent_game_log_data/data_offset_1800.html\n",
      "Downloaded and moved data for offset 2000 to individual_opponent_game_log_data/data_offset_2000.html\n",
      "No more data at offset 2200. Exiting loop.\n",
      "Data extraction completed.\n",
      "Runtime: 3.98 minutes\n",
      "Code execution completed at: 2024-12-05 16:13:50\n",
      "Downloaded and moved data for offset 0 to individual_advanced_game_log_data/data_offset_0.html\n",
      "Downloaded and moved data for offset 200 to individual_advanced_game_log_data/data_offset_200.html\n",
      "Downloaded and moved data for offset 400 to individual_advanced_game_log_data/data_offset_400.html\n",
      "Downloaded and moved data for offset 600 to individual_advanced_game_log_data/data_offset_600.html\n",
      "Downloaded and moved data for offset 800 to individual_advanced_game_log_data/data_offset_800.html\n",
      "Downloaded and moved data for offset 1000 to individual_advanced_game_log_data/data_offset_1000.html\n",
      "Downloaded and moved data for offset 1200 to individual_advanced_game_log_data/data_offset_1200.html\n",
      "Downloaded and moved data for offset 1400 to individual_advanced_game_log_data/data_offset_1400.html\n",
      "Downloaded and moved data for offset 1600 to individual_advanced_game_log_data/data_offset_1600.html\n",
      "Downloaded and moved data for offset 1800 to individual_advanced_game_log_data/data_offset_1800.html\n",
      "Downloaded and moved data for offset 2000 to individual_advanced_game_log_data/data_offset_2000.html\n",
      "No more data at offset 2200. Exiting loop.\n",
      "Data extraction completed.\n",
      "Runtime: 4.04 minutes\n",
      "Code execution completed at: 2024-12-05 16:17:52\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import shutil\n",
    "from selenium import webdriver\n",
    "from datetime import datetime\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# Start the timer since this takes a bit longer to gather data\n",
    "start_time = time.time()\n",
    "\n",
    "# Specify the download and destination folders\n",
    "download_folder = \"/Users/alexdarrow/Downloads\"\n",
    "destination_folder = \"/Users/alexdarrow/Desktop/programming/jupyter_notebooks/nfl_scraping_project/data_2.0/stathead_individual_game_crawl/individual_team_game_log_data\"\n",
    "destination_folder_abrr = \"individual_team_game_log_data\"\n",
    "\n",
    "# Set up Chrome options to specify download directory\n",
    "chrome_options = webdriver.ChromeOptions()\n",
    "prefs = {\"download.default_directory\": download_folder}\n",
    "chrome_options.add_experimental_option(\"prefs\", prefs)\n",
    "\n",
    "# Set up the WebDriver using ChromeDriverManager\n",
    "service = Service(ChromeDriverManager().install())\n",
    "driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "# Login URL\n",
    "login_url = \"https://stathead.com/users/login.cgi\"\n",
    "driver.get(login_url)\n",
    "\n",
    "# Wait for the login page to load\n",
    "wait = WebDriverWait(driver, 10)\n",
    "\n",
    "# Find the login form elements\n",
    "username_field = wait.until(EC.element_to_be_clickable((By.NAME, \"username\")))\n",
    "password_field = driver.find_element(By.NAME, \"password\")\n",
    "login_button = driver.find_element(By.ID, \"sh-login-button\")\n",
    "\n",
    "# Enter login credentials\n",
    "username_field.send_keys(\"adarro5302@gmail.com\")  # Replace with your username\n",
    "password_field.send_keys(\"Con97set!\")  # Replace with your password\n",
    "\n",
    "# Submit the login form\n",
    "login_button.click()\n",
    "\n",
    "# Wait for successful login\n",
    "time.sleep(5)\n",
    "\n",
    "# Define the base URL for pagination\n",
    "base_url = \"https://stathead.com/football/team-game-finder.cgi?request=1&order_by=date&team_game_max=17&year_max=2024&comp_type=reg&week_num_season_max=22&timeframe=seasons&week_num_season_min=1&team_game_min=1&year_min=2021&match=team_game&cstat[1]=points&ccomp[1]=gt&cval[1]=0&cstat[2]=pass_cmp&ccomp[2]=gt&cval[2]=0&cstat[3]=rush_att&ccomp[3]=gt&cval[3]=0&cstat[4]=tot_yds&ccomp[4]=gt&cval[4]=0&cstat[5]=penalties&ccomp[5]=gt&cval[5]=0&cstat[6]=first_down&ccomp[6]=gt&cval[6]=0&cstat[7]=all_td_team&ccomp[7]=gt&cval[7]=0&cstat[8]=punt&ccomp[8]=gt&cval[8]=0&cstat[9]=kick_ret_td&ccomp[9]=gt&cval[9]=0&cstat[10]=vegas_line&ccomp[10]=gt&cval[10]=-500&offset=\"\n",
    "\n",
    "# Start offset loop\n",
    "# Update this loop offset if the code times out and you need to start up again in the middle of running\n",
    "offset = 0\n",
    "while True:\n",
    "    # Generate URL with the current offset\n",
    "    url = f\"{base_url}{offset}\"\n",
    "    driver.get(url)\n",
    "    time.sleep(5)  # Allow time for the page to load\n",
    "    \n",
    "    # Check if table data exists on the page\n",
    "    try:\n",
    "        export_data_dropdown = wait.until(EC.element_to_be_clickable((By.XPATH, \"//span[text()='Export Data']/ancestor::li[@class='hasmore']\")))\n",
    "    except Exception:\n",
    "        print(f\"No more data at offset {offset}. Exiting loop.\")\n",
    "        break\n",
    "\n",
    "    # Click the dropdown to reveal export options\n",
    "    export_data_dropdown.click()\n",
    "\n",
    "    # Select the \"Get table as Excel Workbook\" button\n",
    "    excel_button = wait.until(EC.element_to_be_clickable((By.XPATH, \"//button[text()='Get as Excel Workbook']\")))\n",
    "    excel_button.click()\n",
    "\n",
    "    # Wait for the download to complete\n",
    "    time.sleep(10)\n",
    "\n",
    "    # Find the most recent file in the download folder\n",
    "    downloaded_file = max([f for f in os.listdir(download_folder)], key=lambda f: os.path.getctime(os.path.join(download_folder, f)))\n",
    "\n",
    "    # Move and rename the downloaded file\n",
    "    file_name_without_extension, file_extension = os.path.splitext(downloaded_file)\n",
    "    new_file_name_with_html = f\"data_offset_{offset}.html\"\n",
    "    shutil.move(os.path.join(download_folder, downloaded_file), os.path.join(destination_folder, new_file_name_with_html))\n",
    "\n",
    "    print(f\"Downloaded and moved data for offset {offset} to {os.path.join(destination_folder_abrr, new_file_name_with_html)}\")\n",
    "\n",
    "    # Increment offset by 200 for the next iteration\n",
    "    offset += 200\n",
    "\n",
    "# Close the browser\n",
    "driver.quit()\n",
    "\n",
    "# End the timer\n",
    "end_time = time.time()\n",
    "\n",
    "print(\"Data extraction completed.\")\n",
    "\n",
    "# Calculate and print runtime\n",
    "runtime = end_time - start_time\n",
    "print(f\"Runtime: {runtime / 60:.2f} minutes\")\n",
    "\n",
    "# Print completing date\n",
    "completion_time = datetime.now()\n",
    "print(f\"Code execution completed at: {completion_time.strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1899d4-bb5b-4062-9b0f-24085e2a3dd1",
   "metadata": {},
   "source": [
    "#### PHASE 1: Stathead_Individual_Game_Crawl ADVANCED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12c1318-20a4-47fa-b291-5889cf09bb09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the timer since this takes a bit longer to gather data\n",
    "start_time = time.time()\n",
    "\n",
    "# Specify the download and destination folders\n",
    "download_folder = \"/Users/alexdarrow/Downloads\"\n",
    "destination_folder = \"/Users/alexdarrow/Desktop/programming/jupyter_notebooks/nfl_scraping_project/data_2.0/stathead_individual_game_crawl/individual_advanced_game_log_data\"\n",
    "destination_folder_abrr = \"individual_advanced_game_log_data\"\n",
    "\n",
    "# Set up Chrome options to specify download directory\n",
    "chrome_options = webdriver.ChromeOptions()\n",
    "prefs = {\"download.default_directory\": download_folder}\n",
    "chrome_options.add_experimental_option(\"prefs\", prefs)\n",
    "\n",
    "# Set up the WebDriver using ChromeDriverManager\n",
    "service = Service(ChromeDriverManager().install())\n",
    "driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "# Login URL\n",
    "login_url = \"https://stathead.com/users/login.cgi\"\n",
    "driver.get(login_url)\n",
    "\n",
    "# Wait for the login page to load\n",
    "wait = WebDriverWait(driver, 10)\n",
    "\n",
    "# Find the login form elements\n",
    "username_field = wait.until(EC.element_to_be_clickable((By.NAME, \"username\")))\n",
    "password_field = driver.find_element(By.NAME, \"password\")\n",
    "login_button = driver.find_element(By.ID, \"sh-login-button\")\n",
    "\n",
    "# Enter login credentials\n",
    "username_field.send_keys(\"adarro5302@gmail.com\")  # Replace with your username\n",
    "password_field.send_keys(\"Con97set!\")  # Replace with your password\n",
    "\n",
    "# Submit the login form\n",
    "login_button.click()\n",
    "\n",
    "# Wait for successful login\n",
    "time.sleep(5)\n",
    "\n",
    "# Define the base URL for pagination\n",
    "base_url = \"https://stathead.com/football/team-game-finder.cgi?request=1&year_min=2021&match=team_game&comp_type=reg&team_game_max=17&team_game_min=1&week_num_season_min=1&year_max=2024&timeframe=seasons&order_by=date&week_num_season_max=22&cstat[1]=pass_target_yds&ccomp[1]=gt&cval[1]=-600&cstat[2]=pass_batted_passes&ccomp[2]=gt&cval[2]=-600&cstat[3]=pocket_time&ccomp[3]=gt&cval[3]=-600&cstat[4]=pass_rpo&ccomp[4]=gt&cval[4]=-600&cstat[5]=rec_yac&ccomp[5]=gt&cval[5]=-600&cstat[6]=rush_yds_before_contact&ccomp[6]=gt&cval[6]=-600&cstat[7]=rush_yds_diff&ccomp[7]=gt&cval[7]=-600&offset=\"\n",
    "\n",
    "# Start offset loop\n",
    "# Update this loop offset if the code times out and you need to start up again in the middle of running\n",
    "offset = 0\n",
    "while True:\n",
    "    # Generate URL with the current offset\n",
    "    url = f\"{base_url}{offset}\"\n",
    "    driver.get(url)\n",
    "    time.sleep(5)  # Allow time for the page to load\n",
    "    \n",
    "    # Check if table data exists on the page\n",
    "    try:\n",
    "        export_data_dropdown = wait.until(EC.element_to_be_clickable((By.XPATH, \"//span[text()='Export Data']/ancestor::li[@class='hasmore']\")))\n",
    "    except Exception:\n",
    "        print(f\"No more data at offset {offset}. Exiting loop.\")\n",
    "        break\n",
    "\n",
    "    # Click the dropdown to reveal export options\n",
    "    export_data_dropdown.click()\n",
    "\n",
    "    # Select the \"Get table as Excel Workbook\" button\n",
    "    excel_button = wait.until(EC.element_to_be_clickable((By.XPATH, \"//button[text()='Get as Excel Workbook']\")))\n",
    "    excel_button.click()\n",
    "\n",
    "    # Wait for the download to complete\n",
    "    time.sleep(10)\n",
    "\n",
    "    # Find the most recent file in the download folder\n",
    "    downloaded_file = max([f for f in os.listdir(download_folder)], key=lambda f: os.path.getctime(os.path.join(download_folder, f)))\n",
    "\n",
    "    # Move and rename the downloaded file\n",
    "    file_name_without_extension, file_extension = os.path.splitext(downloaded_file)\n",
    "    new_file_name_with_html = f\"data_offset_{offset}.html\"\n",
    "    shutil.move(os.path.join(download_folder, downloaded_file), os.path.join(destination_folder, new_file_name_with_html))\n",
    "\n",
    "    print(f\"Downloaded and moved data for offset {offset} to {os.path.join(destination_folder_abrr, new_file_name_with_html)}\")\n",
    "\n",
    "    # Increment offset by 200 for the next iteration\n",
    "    offset += 200\n",
    "\n",
    "# Close the browser\n",
    "driver.quit()\n",
    "\n",
    "# End the timer\n",
    "end_time = time.time()\n",
    "\n",
    "print(\"Data extraction completed.\")\n",
    "\n",
    "# Calculate and print runtime\n",
    "runtime = end_time - start_time\n",
    "print(f\"Runtime: {runtime / 60:.2f} minutes\")\n",
    "\n",
    "# Print completing date\n",
    "completion_time = datetime.now()\n",
    "print(f\"Code execution completed at: {completion_time.strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710841cb-b35f-47d5-aaa4-6814594aa77f",
   "metadata": {},
   "source": [
    "#### PHASE 1: Stathead_Individual_Game_Crawl OPPONENT (IP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b49d6ee-7599-4044-850f-6d32c666565d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the timer since this takes a bit longer to gather data\n",
    "start_time = time.time()\n",
    "\n",
    "# Specify the download and destination folders\n",
    "download_folder = \"/Users/alexdarrow/Downloads\"\n",
    "destination_folder = \"/Users/alexdarrow/Desktop/programming/jupyter_notebooks/nfl_scraping_project/data_2.0/stathead_individual_game_crawl/individual_opponent_game_log_data\"\n",
    "destination_folder_abrr = \"individual_opponent_game_log_data\"\n",
    "\n",
    "# Set up Chrome options to specify download directory\n",
    "chrome_options = webdriver.ChromeOptions()\n",
    "prefs = {\"download.default_directory\": download_folder}\n",
    "chrome_options.add_experimental_option(\"prefs\", prefs)\n",
    "\n",
    "# Set up the WebDriver using ChromeDriverManager\n",
    "service = Service(ChromeDriverManager().install())\n",
    "driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "# Login URL\n",
    "login_url = \"https://stathead.com/users/login.cgi\"\n",
    "driver.get(login_url)\n",
    "\n",
    "# Wait for the login page to load\n",
    "wait = WebDriverWait(driver, 10)\n",
    "\n",
    "# Find the login form elements\n",
    "username_field = wait.until(EC.element_to_be_clickable((By.NAME, \"username\")))\n",
    "password_field = driver.find_element(By.NAME, \"password\")\n",
    "login_button = driver.find_element(By.ID, \"sh-login-button\")\n",
    "\n",
    "# Enter login credentials\n",
    "username_field.send_keys(\"adarro5302@gmail.com\")  # Replace with your username\n",
    "password_field.send_keys(\"Con97set!\")  # Replace with your password\n",
    "\n",
    "# Submit the login form\n",
    "login_button.click()\n",
    "\n",
    "# Wait for successful login\n",
    "time.sleep(5)\n",
    "\n",
    "# Define the base URL for pagination\n",
    "base_url = \"https://stathead.com/football/team-game-finder.cgi?request=1&team_game_max=17&week_num_season_min=1&timeframe=seasons&match=team_game&year_max=2024&order_by=date&year_min=2021&team_game_min=1&week_num_season_max=22&comp_type=reg&cstat[1]=pass_cmp_opp&ccomp[1]=gt&cval[1]=0&cstat[2]=rush_att_opp&ccomp[2]=gt&cval[2]=0&cstat[3]=tot_yds_opp&ccomp[3]=gt&cval[3]=0&cstat[4]=first_down_opp&ccomp[4]=gt&cval[4]=0&cstat[5]=all_td_opp&ccomp[5]=gt&cval[5]=0&cstat[6]=def_tgt_yds_per_att&ccomp[6]=gt&cval[6]=0&offset=\"\n",
    "\n",
    "# Start offset loop\n",
    "# Update this loop offset if the code times out and you need to start up again in the middle of running\n",
    "offset = 0\n",
    "while True:\n",
    "    # Generate URL with the current offset\n",
    "    url = f\"{base_url}{offset}\"\n",
    "    driver.get(url)\n",
    "    time.sleep(5)  # Allow time for the page to load\n",
    "    \n",
    "    # Check if table data exists on the page\n",
    "    try:\n",
    "        export_data_dropdown = wait.until(EC.element_to_be_clickable((By.XPATH, \"//span[text()='Export Data']/ancestor::li[@class='hasmore']\")))\n",
    "    except Exception:\n",
    "        print(f\"No more data at offset {offset}. Exiting loop.\")\n",
    "        break\n",
    "\n",
    "    # Click the dropdown to reveal export options\n",
    "    export_data_dropdown.click()\n",
    "\n",
    "    # Select the \"Get table as Excel Workbook\" button\n",
    "    excel_button = wait.until(EC.element_to_be_clickable((By.XPATH, \"//button[text()='Get as Excel Workbook']\")))\n",
    "    excel_button.click()\n",
    "\n",
    "    # Wait for the download to complete\n",
    "    time.sleep(10)\n",
    "\n",
    "    # Find the most recent file in the download folder\n",
    "    downloaded_file = max([f for f in os.listdir(download_folder)], key=lambda f: os.path.getctime(os.path.join(download_folder, f)))\n",
    "\n",
    "    # Move and rename the downloaded file\n",
    "    file_name_without_extension, file_extension = os.path.splitext(downloaded_file)\n",
    "    new_file_name_with_html = f\"data_offset_{offset}.html\"\n",
    "    shutil.move(os.path.join(download_folder, downloaded_file), os.path.join(destination_folder, new_file_name_with_html))\n",
    "\n",
    "    print(f\"Downloaded and moved data for offset {offset} to {os.path.join(destination_folder_abrr, new_file_name_with_html)}\")\n",
    "\n",
    "    # Increment offset by 200 for the next iteration\n",
    "    offset += 200\n",
    "\n",
    "# Close the browser\n",
    "driver.quit()\n",
    "\n",
    "# End the timer\n",
    "end_time = time.time()\n",
    "\n",
    "print(\"Data extraction completed.\")\n",
    "\n",
    "# Calculate and print runtime\n",
    "runtime = end_time - start_time\n",
    "print(f\"Runtime: {runtime / 60:.2f} minutes\")\n",
    "\n",
    "# Print completing date\n",
    "completion_time = datetime.now()\n",
    "print(f\"Code execution completed at: {completion_time.strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
